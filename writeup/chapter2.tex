\chapter{Spatially Random Maps}
As studying the effects of random spatial perturbations
on the dynamics of one-dimensional maps may yield implications for
understanding the dynamics on higher dimensional maps, we present two
case studies:
\begin{enumerate}
\item the logistic map,
\item the circle map.
\end{enumerate}
A spatially random process is one that depends on space and is
invariant in time; there is no time dependence in the process. A
function that takes on a random value for any given location in
space, but retains the same value if it were to return to this same
location at a later time would be a spatially random process. The random spatial perturbations in both maps are intended to mimic
the noise used in hydraulic modeling for transmissivity, which is assumed to be log-normal with an
exponentially decaying spatial correlation. Observations
in subsurface hydrology suggest flow parameters are
log-normal~\cite{gelhar}. The following sections offer background
information on the nature of the spatially random processes and the characteristics of both maps with the spatial perturbations. 
\section{Spatially Random Processes}\label{spatprocs}
The noise in hydraulic modeling is often assumed to be log-normal with an
exponentially decaying spatial correlation, so we let
\begin{equation}\label{R}
\xi(x)=\ln(R(x)) 
\end{equation}
be a normal random variable that represents the noise. Then
$R(x)$ is a log-normal random variable~\cite{gelhar}. A normal distribution may be used
for subsurface storage properties\footnote{Some examples of subsurface
storage properties include porosity and moisture content.}, but in some cases, it is
inappropriate because it would imply negative values have nonzero
probability. Alternatively, a log-normal random variable may
be used for physical parameters known to be nonnegative. Moreover, the log-normal distribution frequently appears when observing
nonnegative flow parameters underground, like hydraulic conductivity and
transmissivity~\cite{gelhar}. An important property we would like to
capture in this process is how the noise should vary based on
position. In aquifers, porosity in a neighborhood of rock tends to be
somewhat consistent (highly correlated), whereas porosity between two
areas on opposite ends of the aquifer can be completely
different. Thus, the noise in the random process should depend on the
distance between two locations instead of the locations themselves,
and we call this property homogeneity. 

We begin to characterize the process by
assuming the entire random process $\xi(x)$ is homogeneous, and defining the mean $\mu$ and variance $\sigma^2$
of $\xi(x)$ to be
\begin{align}
\begin{split}\label{ximean}
\mu &= E[\xi(x)] = \ln(r)\\
\sigma^2 &= E[(\xi(x) - \mu)^2]=E[\xi(x)^2]-(\ln(r))^2.
\end{split}
\end{align}
From probability, the definition of covariance is~\cite{ross}
\begin{align*}
\begin{split}
C(\xi(x),\xi(y)) &= E[(\xi(x) - E[\xi(x)])(\xi(y) - E[\xi(y)])]\\
&= E[\xi(x)\xi(y)] - E[\xi(x)]E[\xi(y)].
\end{split}
\end{align*}
Let us compute $C(\xi(y+x),\xi(y))$. Homogeneity implies $E[\xi(x)] =
E[\xi(y)] = E[\xi(y+x)]$.
\begin{align}
\begin{split}\label{cor}
C(\xi(y+x),\xi(y)) &= E[(\xi(y+x) - E[\xi(y+x)])(\xi(y) -
E[\xi(y)])]\\
&= E[(\xi(y+x) -\ln(r))(\xi(y)-\ln(r))]. 
\end{split}
\end{align}
As the entire process is homogeneous, the covariance is also
homogeneous. Indeed, if the covariance function is defined as a function of $x,y$, but the
variability is independent of location in space; rather, dependent on
the separation vector between the locations, we call the covariance
function homogeneous~\cite{gelhar}. Homogeneity is consistent with the
noise in nonnegative flow parameters, which is
thought to rely on the distance between two points instead of their
precise locations. $C$ is also called a correlation function, which
describes how variables at different positions in space are
related. It physically represents the degree of correlation between
two consecutive instances in the process. Correlations are typically strongest
when iterates are nearby, and they decay as the distance
between two iterates increases, although random processes could violate this assumption. 

We construct a random function $\xi(x)$ on $[0,1]$ that satisfies
(\ref{ximean}) with an infinite Fourier series,
\begin{align}\label{fs1}
\xi(x) = \ln(r) + \sum_{n \in \mathbb{Z}}\hat{\xi}_ne^{2\pi inx}.
\end{align}
Desirably, the magnitudes of the Fourier modes $\hat{\xi}_n$ diminish
as $n \to \pm \infty$, which implies the random log-fluctuations are
not identically distributed. If we modify the variance of the modes to
decay exponentially, we achieve convergence in the sum. To ensure~(\ref{fs1}) stays in the real plane, we impose the condition
\begin{align}\label{realcond}
\hat{\xi}_{n}^* = \hat{\xi}_{-n},
\end{align}
where $\hat{\xi}_n=a_n + ib_n \in \mathbb{C}$ and the $*$ operator denotes the complex conjugate. Equation (\ref{fs1}) assumes $a_0=0$
because we wish to maintain $E[\xi(x)]=\ln(r)$ when $n=0$ (the
spatially independent term $a_0$ will be absorbed into $\ln(r)$).
Since the expectation of a sum is equal to the sum of
expectations, we find~\cite{ross}
\begin{align*}
\begin{split}
E[\xi(x)] &= E\left[\ln(r) + \sum_{n \in \mathbb{Z}}\hat{\xi_n}e^{2\pi
  inx}\right]\\
&= \ln(r) + E\left[\sum_{n \in \mathbb{Z}}\hat{\xi_n}e^{2\pi inx}\right]\\
&= \ln(r) + \sum_{n \in \mathbb{Z}}e^{2\pi inx}E[\hat{\xi_n}].
\end{split}
\end{align*}
If we choose the mean of the Fourier modes $\hat{\xi}_n$ to be zero,
then the mean of the Fourier Series is zero, which preserves the expected value of $\xi(x)$ in~(\ref{ximean}). Therefore, we let
\begin{align}
\begin{split}\label{xihatmean}
\hat{\mu}_n&=E[\hat{\xi}_n]=0\\
\hat{\sigma}_n^2&=E[\hat{\xi}_n^2]-(E[\hat{\xi}_n])^2=E[\hat{\xi}_n^2].
\end{split}
\end{align}
If we assume the modes $\hat{\xi}_n$ are independent, then the product of
two modes $n,m$, where $m \neq -n$, have expectation
\begin{align*}
\begin{split}
E[\hat{\xi}_n\hat{\xi}_m]&=E[\hat{\xi}_n]E[\hat{\xi}_m]\\
&=0.
\end{split}
\end{align*}
However, if $m=-n$, 
\begin{align*}
\begin{split}
E[\hat{\xi}_n\hat{\xi}_{-n}]&= E[\hat{\xi}_n\hat{\xi}_n^*]\\
%&=E[a_n^2+b_n^2]\\
&=E[|\hat{\xi}_n|^2]
\end{split}
\end{align*}
which follows from~(\ref{realcond}). In other words,
\begin{displaymath}
E[\hat{\xi}_n\hat{\xi}_m] = \left\{
     \begin{array}{lr}
       E[|\hat{\xi}_n|^2] & : m = -n\\
       0 & : m \neq -n.\\
     \end{array}
   \right.
\end{displaymath} 
Notice we may choose a function of $n$ for
$E[|\hat{\xi}_n|^2]$. Remember that if we choose a function that
causes $\lim_{n \to \infty}\hat{\xi}_n \to 0$ fast enough, then the sum in~(\ref{fs1}) will
converge. For notation, let the function $S:\mathbb{Z} \to \mathbb{R}$ be
\begin{equation}\label{Sn}
S(n) = E[|\hat{\xi}_n|^2] = \hat{\sigma}_n^2.
\end{equation}
Equation~(\ref{Sn}) is also called the spectral density function. We introduce the notion of the spectral density function $S(n)$ as the Fourier transform
of the correlation
function~(\ref{cor})~\cite{gelhar}. From~(\ref{xihatmean}), it is
clear the correlation function is a function of $x$,
\begin{align*}
\begin{split}
C(\xi(x+y),\xi(y)) &= E[(\xi(y+x) -\ln(r))(\xi(y)-\ln(r))]\\
\xi(y) - \ln(r) &= \sum_n\hat{\xi}_ne^{2\pi iny}\\
\xi(y+x) - \ln(r) &= \sum_m\hat{\xi}_ne^{2\pi im(x+y)}\\
C(\xi(x+y),\xi(y)) &= E\left[\sum_n\sum_m \hat{\xi}_n\hat{\xi}_m e^{2\pi
  imx}e^{2\pi iy(n+m)}\right]\\
&= \sum_n\sum_m E\left[\hat{\xi}_n\hat{\xi}_m e^{2\pi
  imx}e^{2\pi iy(n+m)}\right],
\end{split}
\end{align*}
since if $m \neq -n$, then $E[\hat{\xi}_n\hat{\xi}_m]=0$ and if $m =-n$, then $e^{2\pi
  iy(n+m)}=1$, then
\begin{align*}
C(x) = E[|\hat{\xi}_n|^2 e^{-2\pi inx}].
\end{align*}
The Fourier transform of $C(x)$ demonstrates
\begin{align*}
\begin{split}
\hat{C}(k) &= \int_{0}^{1}C(x)e^{-2\pi ikx}dx\\
&= \sum_{m,n} \int_{0}^{1} E[\hat{\xi}_n\hat{\xi}_m] e^{2\pi
  im(x+y)}e^{2\pi iny}e^{-2\pi ikx}dx\\
&=S(k).
\end{split}
\end{align*}
By nature of being defined as the Fourier transform of
the covariance function, the spectrum represents a distribution of variance over
frequency. In fact, for $x=0$, (\ref{cor}) reduces to the definition
of variance of $\xi(x)$, so 
\begin{align*}
\begin{split}
C(x) &= \sum_{n\in \mathbb{Z}}S(n)e^{2\pi inx}\\
C(0) &= \sigma^2 = \sum_{n\in \mathbb{Z}}S(n).
\end{split}
\end{align*}
A simple spectrum that falls off quickly is
\begin{align}\label{spec}
S(n)=\alpha e^{-L|n|},
\end{align}
where $L > 0$ is the correlation length (and is fixed
for each simulation). Values of $L$ close to zero will result in a
small, negative exponent, which consequently increases the
spectrum. On the other hand, larger values of $L$ cause
the spectrum to decay faster. This choice of $S(n)$ corresponds to a correlation function
\begin{align*}
\begin{split}
C(x) &= \sum_{n\in \mathbb{Z}}S(n)e^{2\pi inx}=\sum_{n\in \mathbb{Z}}\alpha \frac{1}{e^{L|n|}}e^{2\pi inx}\\
&= \alpha + \sum_{n=1}^{\infty}\alpha e^{2\pi
  inx-L|n|}+\sum_{n=-1}^{-\infty}\alpha e^{2\pi inx-L|n|}\\
&= \alpha+\alpha \sum_{n=1}^{\infty}(e^{2\pi ix-L})^n+\alpha \sum_{m=1}^{\infty}(e^{-2\pi ix-L})^m\\
&=\alpha + \alpha \frac{e^{2\pi ix-L}}{1-e^{2\pi ix-L}} +\alpha
\frac{e^{-2\pi ix-L}}{1-e^{-2\pi ix-L}}\\
&=\alpha \left(1+ \frac{-2e^{-2L}+2\cos(2\pi x)e^{-L}}{e^{-2L}-2\cos(2\pi x)e^{-L}+1} \right).\\
\end{split}
\end{align*}
Finally,
\begin{align}\label{cx}
C(x)= \alpha \frac{e^{2L}-1}{e^{2L}-2\cos(2\pi x)e^L+1}.
\end{align}
Figure~\ref{fig:covspec} demonstrates an example of a covariance-spectrum
pair from~(\ref{cx}) and~(\ref{spec}). In~(\ref{spec}) and~(\ref{cx}), the parameter $\alpha$ is a constant used to normalize
$C(x)$.
\begin{figure}[htp]
\caption[Example of a covariance-spectrum pair]{The correlation function
  $C(x)$ and the spectral density $S(n)$ for $L=0.5,\sigma=0.0216, \alpha=0.000114$.}\label{fig:covspec}
\centering
\includegraphics[width=.45\textwidth]{figs/cx.png}\hfill
\includegraphics[width=.45\textwidth]{figs/sn.png}
\end{figure}
Recall from~(\ref{cor}) when $x=0$, the covariance function reduces to the definition
of variance, therefore
\begin{align}
\begin{split}\label{a}
\sigma^2&= \alpha \frac{e^{2L}-1}{e^{2L}-2\cos(0)e^L+1}\\
\alpha &=\sigma^2 \frac{e^{2L}-2e^{L} +1}{e^{2L}-1}\\
\alpha &=\sigma^2
\frac{(e^{L}-1)^2}{(e^{L}-1)(e^{L}+1)}\\
\alpha &= \sigma^2 \tanh(L/2).
\end{split}
\end{align}
The parameter $\alpha$ is now defined in terms of the variance
$\sigma^2$ of $\xi(x)$. Examples of the function $C(x)$ are demonstrated in Figure~\ref{fig:correlation}
for various values of $L$.
\begin{figure}[htp]
\caption[The correlation function $C(x)$]{The correlation function
  $C(x)$ for $L \in \{0.1,0.5,1\}$. For small $L$
  (leftmost graph), the correlation is stronger between iterates than for large $L$
  (rightmost graph).}\label{fig:correlation}
\centering
\includegraphics[width=.3\textwidth]{figs/correlation_L01.png}\hfill
\includegraphics[width=.3\textwidth]{figs/correlation_L05.png}\hfill
\includegraphics[width=.3\textwidth]{figs/correlation_L1.png}
\end{figure}

We may wish to bound the distribution of the noise, especially if we
apply the noise to the logistic map. Its parameter $r$ must take on
values in the interval $[0,4]$, or else iterates of the map may leave $[0,1]$. If we
substitute $R(x)$ for the constant parameter $r$, then we require
$R(x) \leq 4$. One way to prevent the possibility of exiting $[0,1]$ is to bound the distribution of $\hat{\xi}_n$ from~(\ref{fs1}). Suppose the probability density
function for $\hat{\xi}_n$ is nonzero only in the complex square centered at the
origin with side length $2M_n$ (shown in Figure~\ref{fig:square}). Thus, $|a_n|,|b_n| \leq M_n$, and for a fixed $r$, we
can bound $\ln(R(x))$ using~(\ref{R}) and~(\ref{fs1}). First, recall
\begin{align*}
\xi(x) = \ln(R(x)),
\end{align*}
and because we want $R(x) \leq 4$, this implies
\begin{align*}
\xi(x) = \ln(R(x)) \leq \ln(4).
\end{align*}
Since we also defined $\xi(x)$ as a series, we have
\begin{align*}
\xi(x) =\ln(R(x))= \ln(r) + \sum_{n \in \mathbb{Z}}\hat{\xi_n}e^{2\pi inx} \leq \ln(r) + \sum_{n \in \mathbb{Z}}|\hat{\xi_n}|,
\end{align*}
where $r$ is fixed for any given realization. Remember the sum
in~(\ref{fs1}) was constructed to be real, so it is less than the sum
over $|\hat{\xi}_n|$. If the sum over the magnitude of the Fourier modes is bounded by a constant, $\xi(x)$ will also be bounded by the same limit; therefore we choose
\begin{align*}
\begin{split}
\ln(r) + \sum_{n \in \mathbb{Z}}|\hat{\xi_n}| &\leq \ln(4)\\
\sum_{n \in \mathbb{Z}}|\hat{\xi_n}| &\leq \ln(4/r).
\end{split}
\end{align*}
We have $|\hat{\xi_n}| = \sqrt{a_n^2 + b_n^2}$ and $|a_n|,|b_n| \leq M_n$, which implies
\begin{align}
\begin{split}\label{bdnr2}
\sum_{n \in \mathbb{Z}}|\hat{\xi_n}| =\sum_{n \in \mathbb{Z}}\sqrt{a_n^2 + b_n^2} &\leq \sum_{n \in
  \mathbb{Z}}|a_n| + |b_n|\\
&\leq \sum_{n \in \mathbb{Z}}2M_n.
\end{split}
\end{align}
What remains is to choose $M_n$ such that the sum in~(\ref{bdnr2}) is
bounded by $\ln(4/r)$,
\begin{align}\label{bdnr}
\sum_{n \in \mathbb{Z}}\hat{\xi_n}e^{2\pi inx} \leq 2\sum_{n \in \mathbb{Z}}M_n \leq \ln(4/r).
\end{align}
In order for this to occur, the sequence of side lengths $M_n$ must be summable. 
\begin{figure}[!h]
\caption[Uniform distribution over a square region]{The probability
  density function of $\hat{\xi_n}$ is uniformly distributed across
  this square region, centered at the origin. The square region has
  side length $2M_n$ and area $4M_n^2$.}\label{fig:square}
	\begin{center}
		\includegraphics[scale=0.7]{figs/square.png}
	\end{center}
\end{figure}

Using a bounded distribution to describe $\hat{\xi}_n$ will ensure
$R(x) \leq 4$, unlike an unbounded distribution. Let $\hat{\xi}_n$ be uniformly distributed, so 
\begin{equation}\label{eq:square}
   h(\hat{\xi}_n) =h(a_n,b_n)= \left\{
     \begin{array}{lr}
       \frac{1}{4 M_n^2} & |a_n|,|b_n| \leq M_n\\
       0 & |a_n|,|b_n| > M_n\\
     \end{array}
   \right.
\end{equation} 
where $h:\mathbb{C}\to [0,1]\in \mathbb{R}$ is the probability density
function of $\hat{\xi}_n$. To find the relationship between $S(n)$ and
$M_n$, recall that $S(n)$ is defined as the variance of the
log-fluctuations~(\ref{Sn}). Then, 
\begin{align}
\begin{split}\label{Mn}
S(n)&=E[|\hat{\xi}_n|^2] = E[a_n^2+b_n^2]\\
 &= \frac{1}{4M_n^2}\int_{-M_n}^{M_n}\int_{-M_n}^{M_n}(a_n^2+b_n^2)\,da_n\,db_n\\
&=\frac{2}{3}M_n^2\\
M_n&=\sqrt{\frac{3}{2}S(n)}.
\end{split}
\end{align}
Finally, using the expression for $\alpha$~(\ref{a}), the
expression for $M_n$~(\ref{Mn}), and the sum from~(\ref{bdnr}), let
\begin{align*}
A = M_0+2\sum_{n=1}^\infty M_n \leq \ln(4/r).
\end{align*}
Then, we find
\begin{align*}
\begin{split}
A &=\left(\sqrt{\frac{3}{2}\alpha} +
2\sqrt{\frac{3}{2}\alpha}\sum_{n=1}^{\infty}e^{-Ln/2}\right) \\
&= \sqrt{\frac{3}{2}}\alpha\left(1+ \frac{2e^{-L/2}}{1-e^{-L/2}} \right)\\
&= \sigma\sqrt{\frac{3}{2}}
\sqrt{\tanh(L/2)}\left(\frac{e^{L/2}+1}{e^{L/2}-1} \right)\\
&= \sigma \sqrt{\frac{3}{2}}\frac{\sqrt{\tanh(L/2)}}{\tanh(L/4)} \leq \ln(4/r).\\
\end{split}
\end{align*}
Thus, the standard deviation $\sigma$ of the variable $\xi(x)$ must be bounded from below by
zero and from above by
\begin{align}\label{sigma}
0<\sigma \leq \ln(4/r)\sqrt{\frac{2}{3}}\frac{\tanh(L/4)}{\sqrt{\tanh(L/2)}}.
\end{align}
Figure~\ref{fig:R} is a sample realization of a function $R(x)$ that
behaves according to~(\ref{sigma}).
\begin{figure}[!h]
\caption[The function $R(x)$]{The function $R:[0,1]\to [0,4]$ where
  $\hat{\xi}_n\sim Unif(-M_n,M_n),\sigma=0.0386, L=0.1, r=3.5,
  N=100$. $N$ is the upper limit on the number of Fourier terms in the
sum from~(\ref{fs1}).}\label{fig:R}
	\begin{center}
		\includegraphics[scale=0.6]{figs/xi.png}
	\end{center}
\end{figure}

The Lyapunov Central Limit Theorem demonstrates that
the sum over independent, non-identical uniform random variables
$\hat{\xi}_n$ tends to the normal distribution if certain conditions
are upheld~\cite{billingsley}. Unfortunately, we were unable to
analytically demonstrate the limiting quantity in the theorem tends to
zero. However, numerical simulations suggest the distribution of
$\hat{\xi}_n$ is very similar to the normal distribution. To clarify,
it remains to be determined whether random function $\xi(x)$ is
normally distributed if $\hat{\xi}_n \sim Unif(-M_n,M_n)$. However, a histogram (Figure~\ref{fig:tmsm}) of the quantity $\frac{T_m}{s_m}$ in~(\ref{eq:tmsm})
suggests there is some similarity to the standard normal
distribution. Therefore, even though a spatially random process based
on uniformly distributed Fourier mode amplitudes is not explicitly
shown to be log-normal distributed, numerical simulations of a
transformation of the process imply it mimics a log-normal distribution.

\begin{singlespacing}
\begin{theorem}\label{clt}
Let $\hat{\xi}_1, \hat{\xi}_2, ...\hat{\xi}_m$ be a sequence of independently distributed random
variables, each having mean $E[\hat{\xi}_n]=\hat{\mu}_n$ and variance
$\hat{\sigma}_n^2 < \infty$. Define
\begin{align*}
\begin{split}
Y_n &=\hat{\xi}_n - \hat{\mu}_n\\
T_m &= \sum_{n=1}^{m}Y_n\\
s_m^2 &=\sum_{n=1}^m\hat{\sigma}_n^2.
\end{split}
\end{align*}
if $\exists\, \delta>0$,
\begin{align*}
\lim_{m\to \infty}\frac{1}{s_m^{2+\delta}}\sum_{n=1}^mE[|Y_n|^{2+\delta}] \to 0,
\end{align*}
then the distribution of
\begin{equation*}
\frac{T_m}{s_m} \sim N(0,1).
\end{equation*}
That is, $\frac{T_m}{s_m}$ tends to the standard normal.
\end{theorem}
\end{singlespacing}

Based on the definitions in Theorem~\ref{clt}, we have for
$\hat{\xi}_n \sim Unif(-M_n,M_n)$
\begin{align*}
\begin{split}
Y_n &= \hat{\xi}_n - \hat{\mu}_n = \hat{\xi}_n\\
T_m &= \hat{\xi}_{-m}+\hat{\xi}_{-m+1}+ ...+\hat{\xi}_{0}+\hat{\xi}_{1}+...+\hat{\xi}_{m}\\
&=
\hat{\xi}_{1}+\hat{\xi}_{1}^*+...+\hat{\xi}_{m}+\hat{\xi}_{m}^*\\
&= \sum_{n=1}^{m} 2a_n,
\end{split}
\end{align*}
since $\hat{\xi}_{0} = 0$. A uniform random variable $X\sim Unif(c,d)$ has variance
$Var(X) = \frac{1}{12}(d-c)^2$, so
\begin{align*}
\begin{split}
Var(\hat{\xi}_n) = \hat{\sigma}_n^2  &= \frac{1}{12}(2M_n)^2\\
&= \frac{1}{3}M_n^2\\
&= \frac{1}{2}\alpha e^{-L|n|}\\
s_m^2 &= \frac{\alpha}{2}\left(e^{-L|-m|}+e^{-L|-m+1|}+...+1+e^{-L}+...+e^{-Lm} \right)\\
&= \frac{\alpha}{2} + \alpha \left(e^{-L}+... + e^{-Lm} \right)\\
&=\frac{\alpha}{2} +\alpha\sum_{n=1}^m e^{-Ln}.
\end{split}
\end{align*}
We check whether the sum $s_m^2$ diverges as $m \to \infty$. After applying the Geometric series identity,
\begin{align*}
\begin{split}
\lim_{m\to \infty}s_m^2 &=\frac{\alpha}{2} +\alpha\sum_{n=1}^\infty e^{-Ln}\\
&= \frac{\alpha}{2} + \left( \frac{\alpha}{1-e^{-L}}- \alpha\right)\\
&= \frac{\alpha(e^L+1)}{2(e^L-1)}\\
&= \frac{\alpha}{2 \tanh(L/2)},
\end{split}
\end{align*}
we find the sum does indeed converge to a finite value. Using the
expressions derived above, we find 
\begin{align}\label{eq:tmsm}
\frac{T_m}{s_m} = \frac{\sum_{n=1}^{m} 2a_n}{\sqrt{\frac{\alpha}{2} +\alpha\sum_{n=1}^m e^{-Ln}}}. 
\end{align}
A histogram in Figure~\ref{fig:tmsm} offers some insights on how the
quantity in~(\ref{eq:tmsm}) is distributed.
\begin{figure}[!h]
\caption[A histogram of $\frac{T_m}{s_m}$]{A histogram of
  $\frac{T_m}{s_m}$ from~(\ref{eq:tmsm}), where $L=0.1,N = 100, r =
  3.3, \sigma=0.0176$. The results of 1000 simulations are shown. The probability
  density function of the standard normal distribution is
overlaid in red.}\label{fig:tmsm}
	\begin{center}
		\includegraphics[scale=0.6]{figs/tmsm.png}
	\end{center}
\end{figure}

In summary, for each realization of the random map, the noise
$\xi(x)=\ln(R(x))$ is a spatially random process whose mean value is
$E[\xi(x)]=\ln(r)$. The Fourier modes of $\xi(x)$ are uniform random
variables $a_n,b_n \sim Unif(-M_n,M_n)$, where $M_n=
\sqrt{\frac{3}{2}S(n)}$. The function $S(n)=\alpha e^{-L|n|}$
represents the spectral density, which was chosen to decay exponentially fast. The constant $\alpha = \sigma^2 \tanh(L/2)$ normalizes the corresponding
correlation function $C(x)$. If we wish to restrict $R(x)$
to $[0,4]$, then the standard deviation $\sigma$ of
the noise $\xi(x)$ must be bound according to~(\ref{sigma}). The
log-normal property of the spatially random process may be lost if we
choose $\hat{\xi}_n \sim Unif(-M_n,M_n)$.  

% Since
% $|\hat{\xi}_n| = \sqrt{a_n^2 + b_n^2} \leq |a_n|+|b_n| \leq 2M_n$
% almost surely, we have
% \begin{align*}
% |Y_n|^{2+\delta} = |\hat{\xi}_n|^{2+\delta} \leq (2M_n)^\delta|Y_n|^2.
% \end{align*}
% If we choose $\delta = 1$, this implies
% \begin{align*}
% \begin{split}
% \frac{1}{s_m^{2+\delta}}\sum_{n=1}^mE[|Y_n|^{2+\delta}] \leq
% \frac{1}{s_m^{3}}\sum_{n=1}^m2M_nE[|Y_n|^{2}]
% &=\frac{2}{s_m^{3}}\sum_{n=-m}^m\sqrt{\frac{3}{2}S(n)}S(n)\\
% &=\frac{\sqrt{6}}{s_m^{3}}\sum_{n=-m}^m(\alpha e^{-L|n|})^{3/2}\\
% %&=\frac{\sqrt{6}}{s_m^{3}}\alpha^{3/2}\left(e^{-(3/2)L|-m|}+...+1+...+e^{-(3/2)Lm}\right)\\
% %&=\frac{\sqrt{6}\alpha^{3/2}\left(1+2\sum_{n=1}^{m}e^{-(3/2)Ln}\right)}{s_m^{3}}\\
% &=\frac{\sqrt{6}\alpha^{3/2}\left(1+2\sum_{n=1}^{m}e^{-(3/2)Ln}\right)}{\alpha^{3/2}(\frac{1}{2} +\sum_{n=1}^m e^{-Ln})^{3/2}}\\
% %&=\frac{2\sqrt{6}\left(\frac{1}{2}+(\frac{1-e^{-(3/2)L(m+1)}}{1-e^{-(3/2)L}}-1)\right)}{(\frac{1}{2} +(\frac{1-e^{-L(m+1)}}{1-e^{-L}}-1))^{3/2}}\\
% &=\frac{2\sqrt{6}\left(\frac{1-e^{-(3/2)L(m+1)}}{1-e^{-(3/2)L}}-\frac{1}{2}\right)}{(\frac{1-e^{-L(m+1)}}{1-e^{-L}}-\frac{1}{2})^{3/2}}.
% \end{split}
% \end{align*}
% Finally,
% \begin{align}\label{eq:lyapsum}
% \frac{1}{s_m^{3}}\sum_{n=1}^m2M_nE[|Y_n|^{2}]=\frac{4 \sqrt{3} \left(e^{L/2}+1\right) e^{\frac{1}{2} (-3) L (m+1)} \left(e^{\frac{3}{2} L (m+1)}+e^{\frac{3}{2} L (m+2)}-2 e^{\frac{3 L}{2}}\right)}{\left(e^L-1\right) \left(e^{L/2}+e^L+1\right) \left(\frac{e^{-L (m+1)} \left(e^{L (m+1)}+e^{L (m+2)}-2 e^L\right)}{e^L-1}\right)^{3/2}}.
% \end{align}
% Figure~\ref{fig:lyapsum} demonstrates a plot of the quantity
% in~(\ref{eq:lyapsum}) for a range of $L$ values.
% \begin{figure}[!h]
% \caption[The sum in the Lyapunov Central Limit Theorem]{The sum
%   from~(\ref{eq:lyapsum}) for $m=1000$, where $L=0.7$ (left), $L=1.1$ (middle), and $L=2.7$ (right).}\label{fig:lyapsum}
% 	\begin{center}
% 		\includegraphics[width=.33\textwidth]{figs/lyapunov_clt_L_07.png}\hfill
% \includegraphics[width=.33\textwidth]{figs/lyapunov_clt_L_11.png}\hfill
% \includegraphics[width=.33\textwidth]{figs/lyapunov_clt_L_27.png}
% 	\end{center}
% \end{figure}
% Unfortunately, Figure~\ref{fig:lyapsum} seems to imply that the quantity
% in~(\ref{eq:lyapsum}) does not decay to zero for all values of $L$ as
% $m \to \infty$.

% \begin{align*}
% \begin{split}
% \lim_{m\to
%   \infty}\frac{1}{s_m^{2+\delta}}\sum_{n=1}^mE[|Y_n|^{2+\delta}] \leq \lim_{m\to
%   \infty}\frac{1}{s_m^{3}}2\sum_{n=1}^mM_nE[|Y_n|^2] &= \lim_{m\to \infty}\frac{2\sqrt{6}\left(\frac{1}{2}+\sum_{n=1}^{m}e^{-(3/2)Ln}\right)}{\alpha^{3/2}(\frac{1}{2} +\sum_{n=1}^m e^{-Ln})^3}\\
% &\to 0.
% \end{split}
% \end{align*}
% It is convenient to calculate the moments of a random variable $X$ as a
% way of finding $E[X^n]$. The $k$th moment of a uniform random variable $X \sim
% Unif(c,d)$ is defined as 
% \begin{align*}
% m_k = \frac{1}{k+1}\sum_{i=0}^kc^id^{k-i}.
% \end{align*}
% Thus, the second moment is
% \begin{align*}
% \begin{split}
%   m_2 &=E[\hat{\xi}_n^2]= \frac{c^2+cd+d^2}{3}\\
% &= \frac{M_n^2}{3}.
% \end{split}
% \end{align*}
% If we show $|Y_n|\leq \epsilon s_m$, for $m \to \infty$, then $I\{|Y_n|\geq \epsilon
% s_m\}=0$, and the sum will tend to zero, as desired. We find
% expressions for $|Y_n|$ and $\epsilon s_m$,
% \begin{align*}
% \begin{split}
% \epsilon s_m &= \epsilon \sqrt{\frac{\alpha}{2 \tanh(L/2)}} = \epsilon
% \sqrt{\frac{\alpha}{2}} \sqrt{\frac{e^L+1}{e^L-1}}\\
% |Y_n|&= \sqrt{|\hat{\xi}_n|^2} = \sqrt{a_n^2+b_n^2} \leq \sqrt{2}M_n
% \leq \sqrt{2\frac{3}{2}S(n)}\\
% &\leq \sqrt{3\alpha} e^{-L|n|/2}.
% \end{split}
% \end{align*}
% Let $\hat{\epsilon} = \epsilon \sqrt{1/6}$. Then,
% \begin{align*}
% \begin{split}
% \hat{\epsilon} &\leq \hat{\epsilon}(e^L+1)\\
% \hat{\epsilon}\frac{1}{e^L-1} &\leq
% \hat{\epsilon}\frac{e^L+1}{e^L-1}\\
% \hat{\epsilon}\frac{1}{e^{Ln}} \leq \hat{\epsilon}\frac{1}{e^{Ln}-1} &\leq \hat{\epsilon}\frac{e^L+1}{e^L-1}\\
% e^{-L|n|/2}&\leq \hat{\epsilon}\sqrt{\frac{e^L+1}{e^L-1}} 
% \end{split}
% \end{align*}
\section{Random Dynamics of the Logistic Map}
The spatially random logistic map has not been as well explored in the
literature as the temporally random case~\cite{athreya}. Athreya and
Dai explore the concept of a time-varying logistic map, where the
parameter $r_i$ is a function of each time step, as opposed to position
in space. At each time step, some $r_i \in \{r\}_0^\infty$ is applied
to the map, where the
sequence $\{r\}_0^\infty$ has independent and identically distributed
random variables in [0,4]. In contrast, the spatially random logistic
map replaces the parameter $r$ from~(\ref{logmap}) with a random
function of space, $R:[0,1]\to [0,4]$, and the random variables $\hat{\xi}_n$ are
non-identical. In line with the deterministic map, 
$f:[0,1]\to [0,1]$, where
\begin{equation}\label{randlogmap}
x_{n+1} = f(x_n) = R(x_n)x_n(1-x_n).
\end{equation}
Numerical simulations of~(\ref{randlogmap}) require a discrete limit
on the sum in~(\ref{fs1}). We can combine
the equations in~(\ref{fs1}) and~(\ref{realcond}) to arrive at the following form for
$\xi(x)$,
\begin{align}\label{fs2}
\xi(x) = \ln(r) + 2\sum^N_{n=1}a_n\cos(2\pi nx)-b_n\sin(2\pi nx),
\end{align}
where $N$ represents the number of Fourier modes in the sum, and is an
upper limit that we impose on the summation\footnote{In practice, $N
  \approx 10/L$, where $L$ represents the correlation length. $N$
  should be chosen to be large enough such that the Fourier amplitudes
may decay as more terms are included. This restriction will ensure the
fluctuations are log-normal.}. Figure~\ref{fig:rlogstable} demonstrates one realization of a spatially random logistic map.
\begin{figure}[!h]
\caption[Random logistic map, stable orbit]{One instance of a random
  logistic map (blue), which was iterated for 1000 steps. The map
  dynamics have converged to a stable period 4 orbit (green). Notice the
  ``wiggliness'' in the parabola shape. $r=3.5,L=0.05,N=200,\sigma=0.0061$}\label{fig:rlogstable}
	\begin{center}
		\includegraphics[scale=0.7]{figs/rand_cobweb.png}
	\end{center}
\end{figure}
 Figure~\ref{fig:envelope} depicts rough estimates\footnote{The
  maxima and minima over 500 samples were recorded and plotted in
  these figures.} of upper and lower bounds for the random logistic
map for various sets of parameters. 
\begin{figure}[htp]
\caption[Upper and lower bounds on the random logistic map]{A coarse
  demonstration (500 samples) of the upper and lower bounds of the random logistic
  map. Sample realizations are shown in red. From left to right:
  $\{\sigma=0.1093,r=2.7,L=0.9,N=112\}, \{\sigma=0.0086,r=3.5,L=0.05,N=200\},\{\sigma=0.0071,r=3.7,L=0.1,N=100\}$
  }\label{fig:envelope}
\centering
\includegraphics[width=.3\textwidth]{figs/envelope_500_r27_L09.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_500_r35_L005.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_500_r37_L01.png}
\end{figure}

\section{Random Dynamics of the Circle Map}
We will investigate a spatially random circle map of the form
\begin{align}\label{randcirc}
x_{n+1}= F(x_n) =  x_n + \Omega(x_n) - \frac{k}{2\pi}\sin(2\pi x_n),
\end{align}
where $\omega$ from~(\ref{detcirc}) has been replaced with a function
of space, $\Omega:S^1\to S^1$. To distinguish between the logistic map
and the circle map, we denote the function $R(x)$ in~(\ref{R}) as
$\Omega(x)$; that is, $\Omega(x)$ is also meant to be a log-normal random
variable. In Figure~\ref{fig:rcircstable}, a realization of the
random circle map using the probability density function $h$
from~(\ref{eq:square}) is shown, where the orbit converges to a period 5 orbit. 
\begin{figure}[!h]
\caption[Random circle map, stable orbit under the uniform distribution]{The cobweb
  diagram (green) for a random realization of the circle map (blue) with $\omega =
  0.3, k=1, \hat{\xi}_n \sim Unif(-M_n,M_n), \alpha = 10^{-5},L=0.1, N=100$. The orbit
  has converged to a stable period 5 orbit. Its basin of attraction is
  $S^1$. Transient iterations were removed.}\label{fig:rcircstable}
	\begin{center}
		\includegraphics[scale=0.7]{figs/randcirc_cobweb.png}
	\end{center}
\end{figure}
Figure~\ref{fig:circ_u_envelope1}, \ref{fig:circ_u_envelope2}, and \ref{fig:circ_u_envelope3} depict rough estimates\footnote{The
  maxima and minima over 500 samples were recorded and plotted in
  these figures.} of upper and lower bounds for the random circle map
for various sets of parameters, where the random variables
$a_n,b_n\sim Unif(-M_n,M_n)$. 
\begin{figure}[htp]
\caption[Upper and lower bounds on the random circle map, with a uniform
distribution, where $k=1,\omega=0.3$]{A coarse
  demonstration (500 samples) of the upper (blue) and lower (black)
  bounds of the random circle map, where $\hat{\xi}_n\sim
  Unif(-M_n,M_n),k=1,\omega=0.3,\alpha = 10^{-5},N=100$. Sample
  realizations are shown in red. From left to right: $L=0.9,L=0.5,L=0.1$.}\label{fig:circ_u_envelope1}
\centering
\includegraphics[width=.3\textwidth]{figs/envelope_unif_500_k1_L09_w03.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_unif_500_k1_L05_w03.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_unif_500_k1_L01_w03.png}
\end{figure}

\begin{figure}[htp]
\caption[Upper and lower bounds on the random circle map, with a uniform
distribution, where $k=1.5,\omega=0.3$]{A coarse
  demonstration (500 samples) of the upper (blue) and lower (black)
  bounds of the random circle map, where $\hat{\xi}_n\sim Unif(-M_n,M_n)$
  and $k=1.5,\omega=0.3,\alpha = 10^{-5},N=100$. From left to right:
  $L=0.9,L=0.5,L=0.1$.}\label{fig:circ_u_envelope2}
\centering
\includegraphics[width=.3\textwidth]{figs/envelope_unif_500_k15_L09_w03.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_unif_500_k15_L05_w03.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_unif_500_k15_L01_w03.png}
\end{figure}
\begin{figure}[htp]
\caption[Upper and lower bounds on the random circle map, with a uniform
distribution, where $k=1.5,\omega=0.7$]{A coarse
  demonstration (500 samples) of the upper (blue) and lower (black)
  bounds of the random circle map, where $\hat{\xi}_n\sim
  Unif(-M_n,M_n),k=1.5,\omega=0.7,\alpha = 10^{-5},N=100$. From left
  to right: $L=0.9,L=0.5,L=0.1$.}\label{fig:circ_u_envelope3}
\centering
\includegraphics[width=.3\textwidth]{figs/envelope_unif_500_k15_L09_w07.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_unif_500_k15_L05_w07.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_unif_500_k15_L01_w07.png}
\end{figure}

Numerical simulations of~(\ref{randcirc}) require a finite number of
terms in the Fourier series of $\xi(x)$ in~(\ref{fs1}). Therefore, we find
\begin{align}\label{fs_circ}
\xi(x) = \ln(\omega) + 2\sum^N_{n=1}a_n\cos(2\pi nx)-b_n\sin(2\pi nx).
\end{align}
However, since we
do not require $\Omega(x)$ of the circle map~(\ref{randcirc}) to be bounded as strictly
as $R(x)$ of the logistic map~(\ref{randlogmap}), $\alpha$ may be
chosen freely, and $h(\hat{\xi}_n)$ is not limited to the bounded uniform
distribution. The choice of $h(\hat{\xi}_n)$ should be chosen such that the
sum over the random variables is normal, as we aim to mimic log-normal
noise.

For example, consider choosing $\hat{\xi}_n$ according to a normal
distribution, with mean $\hat{\mu}_n=0$ and variance $\hat{\sigma}_n^2 =
S(n) = \alpha e^{-L|n|}$ for some $\alpha > 0$. In other words, 
\begin{align}\label{eq:norm}
\hat{\xi}_n\sim N(0,\alpha e^{-L|n|}). 
\end{align}
Figure~\ref{fig:Omega} is a plot of an example of the spatially random process,
$\Omega(x)$.
\begin{figure}[htp]
\caption[The function $\Omega(x)$]{The function $\Omega:[0,1]\to [0,4]$ where
  $\alpha=10^{-5}, L=0.1, \omega=0.7, N=100$, and $\hat{\xi}_n \sim N(0,\alpha e^{-L|n|})$.
  }\label{fig:Omega}
\centering
\includegraphics[scale=0.65]{figs/Omega.png}
\end{figure}
Figure~\ref{fig:rcircstable_norm} demonstrates a realization of a random circle
map, where the noise is drawn from the normal distribution.
\begin{figure}[htp]
\caption[Random circle map, stable orbit under the normal distribution]{The cobweb
  diagram (green) for a random realization of the circle map (blue) with $\omega =
  0.3, k=1, \alpha = 10^{-5}, L=0.1, N=100, \hat{\xi}_n \sim N(0,\alpha e^{-L|n|})$. The orbit has converged to a stable period 4 orbit.}\label{fig:rcircstable_norm}
	\begin{center}
		\includegraphics[scale=0.65]{figs/randcirc_norm_cobweb.png}
	\end{center}
\end{figure}
Figure~\ref{fig:circ_n_envelope1}, \ref{fig:circ_n_envelope2}, and \ref{fig:circ_n_envelope3} depict rough estimates\footnote{The
  maxima and minima over 500 samples were recorded and plotted in
  these figures.} of upper and lower bounds for the random circle map
for various sets of parameters, where the random variables
$a_n,b_n\sim N(0,\alpha e^{-L|n|})$.
\begin{figure}[htp]
\caption[Upper and lower bounds on the random circle map, with a normal
distribution, where $k=1,\omega=0.3$]{A coarse
  demonstration (500 samples) of the upper (blue) and lower (black)
  bounds of the random circle map, where $\hat{\xi}_n\sim N(0,\alpha
  e^{-L|n|}),k=1,\omega=0.3,\alpha = 10^{-5},N=100$. Sample
  realizations are shown in red. From left to right: $L=0.9,L=0.5,L=0.1$.}\label{fig:circ_n_envelope1}
\centering
\includegraphics[width=.3\textwidth]{figs/envelope_norm_500_k1_L09_w03.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_norm_500_k1_L05_w03.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_norm_500_k1_L01_w03.png}
\end{figure}
\begin{figure}[htp]
\caption[Upper and lower bounds on the random circle map, with a normal
distribution, where $k=1.5,\omega=0.3$]{A coarse
  demonstration (500 samples) of the upper (blue) and lower (black)
  bounds of the random circle map, where $\hat{\xi}_n\sim N(0,\alpha
  e^{-L|n|}),k=1.5,\omega=0.3,\alpha = 10^{-5},N=100$. From left to
  right: $L=0.9,L=0.5,L=0.1$.}\label{fig:circ_n_envelope2}
\centering
\includegraphics[width=.3\textwidth]{figs/envelope_norm_500_k15_L09_w03.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_norm_500_k15_L05_w03.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_norm_500_k15_L01_w03.png}
\end{figure}
\begin{figure}[htp]
\caption[Upper and lower bounds on the random circle map, with a normal
distribution, where $k=1.5,\omega=0.7$]{A coarse
  demonstration (500 samples) of the upper (blue) and lower (black)
  bounds of the random circle map, where $\hat{\xi}_n\sim N(0,\alpha
  e^{-L|n|}),k=1.5,\omega=0.7,\alpha = 10^{-5},N=100$. From left to
  right: $L=0.9,L=0.5,L=0.1$.}\label{fig:circ_n_envelope3}
\centering
\includegraphics[width=.3\textwidth]{figs/envelope_norm_500_k15_L09_w07.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_norm_500_k15_L05_w07.png}\hfill
\includegraphics[width=.3\textwidth]{figs/envelope_norm_500_k15_L01_w07.png}
\end{figure}

The sum of $m$ independent normal random variables
$X_1, ..., X_m$ with means $\mu_1, ..., \mu_m$ and variances
$\sigma_1^2, ..., \sigma_m^2$ has the convenient property
\begin{align*}
\sum_{n=1}^mX_n \sim N\left(\sum_{n=1}^m\mu_n, \sum_{n=1}^m\sigma_n^2\right).
\end{align*}
Thus, for $\hat{\xi}_{-N},...,\hat{\xi}_N$, we have
\begin{align}\label{sum:norm}
\sum_{n=-N}^N \hat{\xi}_n \sim N\left(0, \alpha\sum_{n=-N}^N e^{-L|n|}\right).
\end{align}
This implies $\xi(x)$ in~(\ref{fs_circ}) is also normal because it is
a transformation of the sum in~(\ref{sum:norm}). Consequently, the
noise in the circle map $\Omega(x)$ is log-normal, as
desired. 

Overall, we have a much simpler set of equations for the circle map
than for the logistic map since $\Omega(x)$ is unrestricted. For each
realization of the map, we choose a set of random variables $a_n, b_n$
that correspond to the Fourier modes of $\xi(x)$, where $\hat{\xi}_n =
a_n + ib_n$. The variables $a_n, b_n$ may be drawn from an unbounded
distribution, as long as the variance of Fourier modes
$\hat{\sigma}_n^2$ decays according to the spectral density $S(n)$ in~(\ref{Sn}). 